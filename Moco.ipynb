{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def is_image_valid(image_path):\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        img.verify()\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, directory, labeled=True, transform=None):\n",
    "        self.directory = directory\n",
    "        self.video_dirs = sorted([d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))])\n",
    "        self.labeled = labeled\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_dirs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_dir = os.path.join(self.directory, self.video_dirs[idx])\n",
    "        frames = []\n",
    "        try:\n",
    "            for i in range(22):\n",
    "                image_path = os.path.join(video_dir, f\"image_{i}.png\")\n",
    "                image = Image.open(image_path).convert(\"RGB\")\n",
    "                frames.append(ToTensor()(image))\n",
    "            video = torch.stack(frames)\n",
    "        except Exception as e:\n",
    "            print(video_dir)\n",
    "            print(e)\n",
    "            \n",
    "            return None\n",
    "        # for i in range(22):\n",
    "        #     image_path = os.path.join(video_dir, f\"image_{i}.png\")\n",
    "        #     if is_image_valid(image_path):\n",
    "        #       image = Image.open(image_path).convert(\"RGB\")\n",
    "        #       frames.append(ToTensor()(image))\n",
    "        \n",
    "        \n",
    "        if self.labeled:\n",
    "            mask_path = os.path.join(video_dir, \"mask.npy\")\n",
    "            mask_data = np.load(mask_path)\n",
    "            mask = torch.tensor(mask_data.reshape(22, 160, 240), dtype=torch.long)\n",
    "            return video, mask\n",
    "        else:\n",
    "            return video\n",
    "\n",
    "\n",
    "\n",
    "path_to_dataset = \"/dataset/\"\n",
    "train_path = os.path.join(path_to_dataset, \"train\")\n",
    "unlabeled_path = os.path.join(path_to_dataset, \"unlabeled\")\n",
    "val_path = os.path.join(path_to_dataset, \"val\")\n",
    "\n",
    "train_dataset = VideoDataset(train_path, transform=transforms.ToTensor())\n",
    "unlabeled_dataset = VideoDataset(unlabeled_path, labeled=False, transform=transforms.ToTensor())\n",
    "val_dataset = VideoDataset(val_path, transform=transforms.ToTensor())\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "unlabeled_dataloader = DataLoader(unlabeled_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoCo(nn.Module):\n",
    "    def __init__(self, encoder, queue_size=8192, temperature=0.07):\n",
    "        super(MoCo, self).__init__()\n",
    "        self.encoder_k = encoder\n",
    "        self.encoder_q = encoder\n",
    "        self.queue_size = queue_size\n",
    "        self.temperature = temperature\n",
    "\n",
    "        self.register_buffer(\"queue\", torch.randn(128, queue_size))\n",
    "        self.queue = nn.functional.normalize(self.queue, dim=0)\n",
    "\n",
    "        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _momentum_update_key_encoder(self, m=0.999):\n",
    "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
    "            param_k.data = param_k.data * m + param_q.data * (1. - m)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _dequeue_and_enqueue(self, keys):\n",
    "        batch_size = keys.shape[0]\n",
    "\n",
    "        ptr = int(self.queue_ptr)\n",
    "        assert self.queue_size % batch_size == 0, \"queue size should be divisible by batch size\"\n",
    "\n",
    "        self.queue[:, ptr:ptr + batch_size] = keys.t()\n",
    "        ptr = (ptr + batch_size) % self.queue_size\n",
    "\n",
    "        self.queue_ptr[0] = ptr\n",
    "\n",
    "    def forward(self, x_q, x_k):\n",
    "        _, _, q = self.encoder_q(x_q) # Change this line to get the proper tensor output\n",
    "        _, _, k = self.encoder_k(x_k) # Change this line to get the proper tensor output\n",
    "\n",
    "        k = nn.functional.normalize(k, dim=1)\n",
    "        self._dequeue_and_enqueue(k)\n",
    "\n",
    "        q = self.encoder_q(x_q)\n",
    "        q = nn.functional.normalize(q, dim=1)\n",
    "\n",
    "        l_pos = torch.einsum(\"nc,nc->n\", [q, k]).unsqueeze(-1)\n",
    "        l_neg = torch.matmul(q, self.queue.clone().detach())\n",
    "\n",
    "        logits = torch.cat([l_pos, l_neg], dim=1)\n",
    "        logits /= self.temperature\n",
    "\n",
    "        labels = torch.zeros(logits.shape[0], dtype=torch.long).cuda()\n",
    "\n",
    "        return logits, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoEncoder(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(VideoEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=(1, 1, 1))\n",
    "        self.conv3 = nn.Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.relu1(self.conv1(x))\n",
    "        x2 = self.pool(self.relu2(self.conv2(x1)))\n",
    "        x3 = self.pool(self.relu3(self.conv3(x2)))\n",
    "        return x1, x2, x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomHorizontalFlip, Compose\n",
    "\n",
    "def data_augmentation():\n",
    "    return Compose([\n",
    "        RandomHorizontalFlip(p=0.5),\n",
    "        \n",
    "    ])\n",
    "\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "def augment_video(batch_video, augmentation):\n",
    "    augmented_batch_video = []\n",
    "    for video in batch_video:\n",
    "        augmented_video = []\n",
    "        for frame in video:\n",
    "            frame = to_pil_image(frame)  # Convert tensor to PIL Image\n",
    "            augmented_frame = augmentation(frame)\n",
    "            augmented_frame = ToTensor()(augmented_frame)  # Change back to (C, H, W)\n",
    "            augmented_video.append(augmented_frame)\n",
    "        augmented_batch_video.append(torch.stack(augmented_video))\n",
    "    return torch.stack(augmented_batch_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'norm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 34\u001b[0m\n\u001b[1;32m     30\u001b[0m train_sampler \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mSubsetRandomSampler(train_indices)\n\u001b[1;32m     32\u001b[0m unlabeled_train_dataloader \u001b[39m=\u001b[39m DataLoader(unlabeled_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, sampler\u001b[39m=\u001b[39mtrain_sampler, num_workers\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m train_moco(unlabeled_train_dataloader, moco, optimizer)\n",
      "Cell \u001b[0;32mIn[12], line 15\u001b[0m, in \u001b[0;36mtrain_moco\u001b[0;34m(dataloader, model, optimizer, epochs)\u001b[0m\n\u001b[1;32m     12\u001b[0m data_q \u001b[39m=\u001b[39m video\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m data_k \u001b[39m=\u001b[39m augment_video(video, augmentation)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 15\u001b[0m logits, labels \u001b[39m=\u001b[39m moco(data_q, data_k)\n\u001b[1;32m     17\u001b[0m loss \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()(logits, labels)\n\u001b[1;32m     18\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[2], line 33\u001b[0m, in \u001b[0;36mMoCo.forward\u001b[0;34m(self, x_q, x_k)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x_q, x_k):\n\u001b[1;32m     32\u001b[0m     k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_k(x_k)\n\u001b[0;32m---> 33\u001b[0m     k \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mnormalize(k, dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     34\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dequeue_and_enqueue(k)\n\u001b[1;32m     36\u001b[0m     q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_q(x_q)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:4660\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(input, p, dim, eps, out)\u001b[0m\n\u001b[1;32m   4658\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(normalize, (\u001b[39minput\u001b[39m, out), \u001b[39minput\u001b[39m, p\u001b[39m=\u001b[39mp, dim\u001b[39m=\u001b[39mdim, eps\u001b[39m=\u001b[39meps, out\u001b[39m=\u001b[39mout)\n\u001b[1;32m   4659\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 4660\u001b[0m     denom \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mnorm(p, dim, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mclamp_min(eps)\u001b[39m.\u001b[39mexpand_as(\u001b[39minput\u001b[39m)\n\u001b[1;32m   4661\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m \u001b[39m/\u001b[39m denom\n\u001b[1;32m   4662\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'norm'"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = VideoEncoder(in_channels=22).to(device)\n",
    "moco = MoCo(encoder).to(device)\n",
    "optimizer = optim.Adam(moco.parameters(), lr=0.001)\n",
    "\n",
    "def train_moco(dataloader, model, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    augmentation = data_augmentation()\n",
    "    for epoch in range(epochs):\n",
    "        for idx, video in enumerate(dataloader):\n",
    "            data_q = video.to(device)\n",
    "            data_k = augment_video(video, augmentation).to(device)\n",
    "\n",
    "            logits, labels = moco(data_q, data_k)\n",
    "\n",
    "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            model._momentum_update_key_encoder()\n",
    "\n",
    "            if idx % 10 == 0:\n",
    "                print(f\"Epoch: {epoch}, Step: {idx}, Loss: {loss.item()}\")\n",
    "\n",
    "random.seed(0)\n",
    "train_indices = list(range(len(unlabeled_dataset)))\n",
    "random.shuffle(train_indices)\n",
    "train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "\n",
    "unlabeled_train_dataloader = DataLoader(unlabeled_dataset, batch_size=8, sampler=train_sampler, num_workers=1)\n",
    "\n",
    "train_moco(unlabeled_train_dataloader, moco, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetDecoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNetDecoder, self).__init__()\n",
    "        self.up = nn.ConvTranspose3d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, in_channels // 2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(in_channels // 2, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class SegmentationModel(nn.Module):\n",
    "    def __init__(self, encoder, num_classes):\n",
    "        super(SegmentationModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        \n",
    "        self.middle = nn.Sequential(\n",
    "            nn.Conv3d(512, 1024, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(1024, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.up1 = UNetDecoder(1024, 256)\n",
    "        self.up2 = UNetDecoder(512, 128)\n",
    "        self.up3 = UNetDecoder(256, 64)\n",
    "        self.up4 = UNetDecoder(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1, x2, x3, x4, x5 = self.encoder(x)\n",
    "        x = self.middle(x5)\n",
    "        x = self.up1(x, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'moco' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m seg_model \u001b[39m=\u001b[39m SegmentationModel(moco\u001b[39m.\u001b[39mencoder_q, num_classes\u001b[39m=\u001b[39m\u001b[39m24\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      2\u001b[0m seg_optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(seg_model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_segmentation\u001b[39m(dataloader, model, optimizer, epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'moco' is not defined"
     ]
    }
   ],
   "source": [
    "seg_model = SegmentationModel(moco.encoder_q, num_classes=24).to(device)\n",
    "seg_optimizer = optim.Adam(seg_model.parameters(), lr=0.001)\n",
    "\n",
    "def train_segmentation(dataloader, model, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for idx, (data, labels) in enumerate(dataloader):\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(data)\n",
    "\n",
    "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if idx % 10 == 0:\n",
    "                print(f\"Epoch: {epoch}, Step: {idx}, Loss: {loss.item()}\")\n",
    "\n",
    "train_segmentation(train_dataloader, seg_model, seg_optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class UNet3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet3D, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.middle = nn.Sequential(\n",
    "            nn.Conv3d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv3d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(64, out_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        self.down = nn.MaxPool3d(2, stride=2)\n",
    "        self.up = nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.encoder(x)\n",
    "        x2 = self.middle(self.down(x1))\n",
    "        x3 = self.decoder(self.up(x2))\n",
    "        return x3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def mc_jepa_loss(logits, pseudo_labels, alpha=0.1):\n",
    "    pred = F.softmax(logits, dim=1)\n",
    "    pred = pred.view(pred.size(0), pred.size(1), -1)\n",
    "    cluster_centers = torch.zeros((pred.size(1), pred.size(2)), device=logits.device)\n",
    "    \n",
    "    for i in range(pred.size(1)):\n",
    "        cluster_centers[i] = torch.mean(pred[pseudo_labels == i], dim=0)\n",
    "    \n",
    "    joint_entropy = -torch.mean(torch.sum(pred * torch.log(torch.clamp(cluster_centers[pseudo_labels], 1e-10)), dim=1))\n",
    "    cross_entropy = -torch.mean(torch.sum(pred * torch.log(torch.clamp(pred, 1e-10)), dim=1))\n",
    "\n",
    "    loss = (1 - alpha) * cross_entropy + alpha * joint_entropy\n",
    "    return loss\n",
    "\n",
    "def mc_jepa_pseudo_labeling(logits):\n",
    "    pred = F.softmax(logits, dim=1)\n",
    "    pred_np = pred.cpu().numpy().reshape(pred.size(0), pred.size(1), -1)\n",
    "    kmeans = KMeans(n_clusters=pred.size(1), random_state=0)\n",
    "    pseudo_labels = []\n",
    "\n",
    "    for i in range(pred_np.shape[0]):\n",
    "        kmeans.fit(pred_np[i].T)\n",
    "        pseudo_labels.append(kmeans.labels_)\n",
    "\n",
    "    pseudo_labels = np.array(pseudo_labels).reshape(pred.size(0), *pred.size()[2:])\n",
    "    return torch.tensor(pseudo_labels, dtype=torch.long, device=logits.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet3d = UNet3D(1, num_classes).to(device)\n",
    "optimizer = optim.Adam(unet3d.parameters(), lr=0.001)\n",
    "\n",
    "def train_mc_jepa(dataloader, model, optimizer, alpha=0.1, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for idx, data in enumerate(dataloader):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(data)\n",
    "            \n",
    "            pseudo_labels = mc_jepa_pseudo_labeling(logits)\n",
    "            loss = mc_jepa_loss(logits, pseudo_labels, alpha)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if idx % 10 == 0:\n",
    "                print(f\"Epoch: {epoch}, Step: {idx}, Loss: {loss.item()}\")\n",
    "\n",
    "train_mc_jepa(unlabeled_train_dataloader, unet3d, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_finetune(dataloader, model, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for idx, (data, labels) in enumerate(dataloader):\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(data)\n",
    "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if idx % 10 == 0:\n",
    "                print(f\"Epoch: {epoch}, Step: {idx}, Loss: {loss.item()}\")\n",
    "\n",
    "train_finetune(train_dataloader, unet3d, optimizer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
