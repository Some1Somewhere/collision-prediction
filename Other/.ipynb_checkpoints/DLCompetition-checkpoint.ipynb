{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import imageio.v3 as iio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def adjust_video_length(video, desired_frames):\n",
    "    current_frames = video.shape[0]\n",
    "    \n",
    "    if current_frames < desired_frames:\n",
    "        pad_frames = desired_frames - current_frames\n",
    "        padding = torch.zeros((pad_frames, *video.shape[1:]), dtype=video.dtype)\n",
    "        video = torch.cat((video, padding), dim=0)\n",
    "    elif current_frames > desired_frames:\n",
    "        video = video[:desired_frames]\n",
    "    \n",
    "    return video\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def is_image_valid(image_path):\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        img.verify()\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, directory):\n",
    "        self.directory = directory\n",
    "        self.video_dirs = sorted([d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_dirs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_dir = os.path.join(self.directory, self.video_dirs[idx])\n",
    "        frames = []\n",
    "        for i in range(22):\n",
    "            image_path = os.path.join(video_dir, f\"image_{i}.png\")\n",
    "            if is_image_valid(image_path):\n",
    "              image = Image.open(image_path).convert(\"RGB\")\n",
    "              frames.append(ToTensor()(image))\n",
    "        video = torch.stack(frames)\n",
    "        # Adjust video length\n",
    "        desired_frames = 22  \n",
    "        video = adjust_video_length(video, desired_frames)\n",
    "        return video\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, directory):\n",
    "        self.directory = directory\n",
    "        self.video_dirs = sorted([d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_dirs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_dir = os.path.join(self.directory, self.video_dirs[idx])\n",
    "        frames = []\n",
    "        for i in range(22):\n",
    "            image_path = os.path.join(video_dir, f\"image_{i}.png\")\n",
    "            if is_image_valid(image_path):\n",
    "                image = Image.open(image_path).convert(\"RGB\")\n",
    "                frames.append(ToTensor()(image))\n",
    "\n",
    "        mask_path = os.path.join(video_dir, \"mask.npy\")\n",
    "        mask_data = np.load(mask_path)\n",
    "        mask = torch.tensor(mask_data.reshape(22, 160, 240), dtype=torch.long)  # Reshape the mask data\n",
    "        video = torch.stack(frames)\n",
    "        # Adjust video length\n",
    "        desired_frames = 22  \n",
    "        video = adjust_video_length(video, desired_frames)\n",
    "        return video, mask\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def save_generated_masks(generated_masks, output_directory):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    for i, mask in enumerate(generated_masks):\n",
    "        output_path = os.path.join(output_directory, f\"mask_{i}.npy\")\n",
    "        np.save(output_path, mask)\n",
    "\n",
    "\n",
    "def calculate_iou_batch(predictions, masks):\n",
    "    smooth = 1e-6\n",
    "    predictions = predictions.argmax(dim=1)\n",
    "    predictions = predictions > 0.5\n",
    "    masks = masks > 0.5\n",
    "\n",
    "    intersection = (predictions & masks).float().sum()\n",
    "    union = (predictions | masks).float().sum()\n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    return iou.item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretextModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PretextModel, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv3d(22, 32, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n",
    "            nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv3d(64, 32, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose3d(32, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose3d(32, 22, kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_dataset = VideoDataset(\"/scratch/dks7920/Dataset_Student/unlabeled\")\n",
    "labeled_train_dataset = SegmentationDataset(\"/scratch/dks7920/Dataset_Student/train\")\n",
    "labeled_val_dataset = SegmentationDataset(\"/scratch/dks7920/Dataset_Student/val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import trange\n",
    "\n",
    "# Self-supervised learning\n",
    "pretext_model = PretextModel().to(device)\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(pretext_model.parameters(), lr=0.001)\n",
    "\n",
    "unlabeled_dataloader = DataLoader(unlabeled_dataset, batch_size=32, shuffle=True)\n",
    "t = trange(num_epochs)\n",
    "for epoch in t:\n",
    "    start_epoch = time.time()\n",
    "\n",
    "    for batch_idx, videos in enumerate(unlabeled_dataloader):\n",
    "        start_batch = time.time()\n",
    "        videos = videos.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        start_forward = time.time()\n",
    "        predictions = pretext_model(videos)\n",
    "\n",
    "        start_loss = time.time()\n",
    "        loss = criterion(predictions, videos)  # Modify this line based on the pretext task\n",
    "\n",
    "        start_backward = time.time()\n",
    "        loss.backward()\n",
    "\n",
    "        start_optim = time.time()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time = time.time() - start_batch\n",
    "        t.set_postfix(batch_time=batch_time, batch_idx=batch_idx + 1)\n",
    "\n",
    "    epoch_time = time.time() - start_epoch\n",
    "    t.set_postfix(epoch_time=epoch_time, epoch=epoch + 1, refresh=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 30/30 [13:27<00:00, 26.92s/it, epoch_time=23.6, training_loss=0.00187]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "num_samples = 500  # Adjust this to the number of samples you want to use\n",
    "small_unlabeled_dataset = Subset(unlabeled_dataset, range(num_samples))\n",
    "small_unlabeled_dataloader = DataLoader(small_unlabeled_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "pretext_model = PretextModel().to(device)\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(pretext_model.parameters(), lr=0.001)\n",
    "\n",
    "from tqdm import trange\n",
    "import time\n",
    "\n",
    "t = trange(num_epochs)\n",
    "for epoch in t:\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "    start_epoch = time.time()\n",
    "\n",
    "    for batch_idx, videos in enumerate(small_unlabeled_dataloader):\n",
    "        start_batch = time.time()\n",
    "        videos = videos.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        start_forward = time.time()\n",
    "        predictions = pretext_model(videos)\n",
    "\n",
    "        start_loss = time.time()\n",
    "        loss = criterion(predictions, videos)  # Modify this line based on the pretext task\n",
    "\n",
    "        start_backward = time.time()\n",
    "        loss.backward()\n",
    "\n",
    "        start_optim = time.time()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time = time.time() - start_batch\n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        t.set_postfix(batch_time=batch_time, batch_idx=batch_idx + 1, refresh=False)\n",
    "    \n",
    "    avg_train_loss = running_loss / num_batches\n",
    "\n",
    "    epoch_time = time.time() - start_epoch\n",
    "    t.set_description(f\"Epoch {epoch + 1}\")\n",
    "    t.set_postfix(epoch_time=epoch_time, training_loss=avg_train_loss, refresh=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationModel(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super(SegmentationModel, self).__init__()\n",
    "        self.base_model = pretrained_model.encoder\n",
    "        self.segmentation_head = nn.Sequential(\n",
    "            nn.Conv3d(64, 32, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(size=(22, 160, 240), mode=\"nearest\"),\n",
    "            nn.Conv3d(32, 16, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(16, 52, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        x = self.segmentation_head(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100\n",
    "small_train_dataset = Subset(labeled_train_dataset, range(num_samples))\n",
    "small_val_dataset = Subset(labeled_val_dataset, range(num_samples))\n",
    "train_dataloader = DataLoader(small_train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(small_val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(labeled_train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(labeled_val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   3%|▎         | 1/30 [00:16<07:47, 16.11s/it, iou=0.0000, train_loss=1.8500, val_loss=1.1345]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at Epoch 1 with IOU: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5:  17%|█▋        | 5/30 [01:20<06:42, 16.10s/it, iou=0.0000, train_loss=0.4375, val_loss=0.4375]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at Epoch 5 with IOU: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6:  20%|██        | 6/30 [01:36<06:26, 16.12s/it, iou=0.0000, train_loss=0.4181, val_loss=0.4269]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at Epoch 6 with IOU: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13:  43%|████▎     | 13/30 [03:28<04:32, 16.01s/it, iou=0.0003, train_loss=0.3329, val_loss=0.3429]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at Epoch 13 with IOU: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14:  47%|████▋     | 14/30 [03:44<04:16, 16.03s/it, iou=0.0012, train_loss=0.3239, val_loss=0.3478]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at Epoch 14 with IOU: 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15:  50%|█████     | 15/30 [04:00<04:00, 16.06s/it, iou=0.0015, train_loss=0.3440, val_loss=0.3565]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at Epoch 15 with IOU: 0.0015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16:  53%|█████▎    | 16/30 [04:19<03:53, 16.67s/it, iou=0.0080, train_loss=0.3364, val_loss=0.3365]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at Epoch 16 with IOU: 0.0080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17:  57%|█████▋    | 17/30 [04:35<03:35, 16.59s/it, iou=0.0353, train_loss=0.3222, val_loss=0.3302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at Epoch 17 with IOU: 0.0353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18:  60%|██████    | 18/30 [04:51<03:17, 16.48s/it, iou=0.0662, train_loss=0.3082, val_loss=0.3275]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at Epoch 18 with IOU: 0.0662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20:  67%|██████▋   | 20/30 [05:25<02:45, 16.58s/it, iou=0.1098, train_loss=0.3276, val_loss=0.3386]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at Epoch 20 with IOU: 0.1098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24:  80%|████████  | 24/30 [06:28<01:36, 16.05s/it, iou=0.1260, train_loss=0.3116, val_loss=0.3426]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at Epoch 24 with IOU: 0.1260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29:  97%|█████████▋| 29/30 [07:50<00:16, 16.29s/it, iou=0.1492, train_loss=0.3830, val_loss=0.4055]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at Epoch 29 with IOU: 0.1492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 30/30 [08:06<00:00, 16.23s/it, iou=0.0576, train_loss=0.3431, val_loss=0.3225]\n"
     ]
    }
   ],
   "source": [
    "# Supervised fine-tuning\n",
    "segmentation_model = SegmentationModel(pretrained_model=pretext_model).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(segmentation_model.parameters(), lr=0.001)\n",
    "t = trange(num_epochs, position=0, leave=True)\n",
    "best_iou = 0\n",
    "for epoch in t:\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    # Training loop\n",
    "    segmentation_model.train()\n",
    "    for videos, masks in train_dataloader:\n",
    "        videos = videos.to(device)\n",
    "        masks = masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = segmentation_model(videos)\n",
    "        loss = criterion(predictions, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    avg_train_loss = running_loss / num_batches\n",
    "\n",
    "    # Validation loop\n",
    "    segmentation_model.eval()\n",
    "    val_loss = 0\n",
    "    iou_sum = 0\n",
    "    num_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for videos, masks in val_dataloader:\n",
    "            videos = videos.to(device)\n",
    "            masks = masks.to(device)\n",
    "            predictions = segmentation_model(videos)\n",
    "            loss = criterion(predictions, masks)\n",
    "            val_loss += loss.item()\n",
    "            iou_sum += calculate_iou_batch(predictions, masks)\n",
    "            num_batches += 1\n",
    "\n",
    "    # Calculate average validation loss and IoU\n",
    "    avg_val_loss = val_loss / num_batches\n",
    "    avg_iou = iou_sum / num_batches\n",
    "\n",
    "    # Update the progress bar with training and validation information\n",
    "    t.set_description(f\"Epoch {epoch + 1}\")\n",
    "    t.set_postfix(\n",
    "        train_loss=f\"{avg_train_loss:.4f}\",\n",
    "        val_loss=f\"{avg_val_loss:.4f}\",\n",
    "        iou=f\"{avg_iou:.4f}\",\n",
    "        refresh=True,\n",
    "    )\n",
    "    if avg_iou > best_iou:\n",
    "        best_iou = avg_iou\n",
    "        torch.save(segmentation_model.state_dict(), f\"best_model.pth\")\n",
    "        t.write(f\"Best model saved at Epoch {epoch + 1} with IOU: {avg_iou:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def count_classes(dataset):\n",
    "    unique_classes = set()\n",
    "    for _, mask in dataset:\n",
    "        for frame in mask:  # Iterate through each frame of the mask\n",
    "            classes = np.unique(frame.numpy())\n",
    "            unique_classes.update(classes)\n",
    "    return len(unique_classes)\n",
    "\n",
    "num_classes = count_classes(labeled_train_dataset)\n",
    "print(f\"Number of classes: {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_data2 = np.load(\"/scratch/dks7920/Dataset_Student/train/video_301/mask.npy\")\n",
    "print(f\"Mask data shape: {mask_data2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Implementing a custom pretext task\n",
    "class NextFramePredictionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NextFramePredictionModel, self).__init__()\n",
    "        # Use the same encoder architecture as in the original code\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv3d(22, 32, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n",
    "            nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n",
    "        )\n",
    "        \n",
    "        # Use a different decoder architecture for the pretext task\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv3d(64, 32, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose3d(32, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose3d(32, 3, kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Replace the pretext model with the new model\n",
    "pretext_model = NextFramePredictionModel().to(device)\n",
    "criterion = nn.MSELoss().to(device)\n",
    "from tqdm import trange\n",
    "\n",
    "t = trange(num_epochs)\n",
    "for epoch in t:\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "    pretext_model.train()\n",
    "    for videos in small_unlabeled_dataloader:\n",
    "        videos = videos.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        input_videos = videos[:, :-1]\n",
    "        target_videos = videos[:, -1]\n",
    "        predictions = pretext_model(input_videos)\n",
    "        loss = criterion(predictions, target_videos)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_time = time.time() - start_batch\n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        t.set_postfix(batch_time=batch_time, batch_idx=batch_idx + 1, refresh=False)\n",
    "    \n",
    "    avg_train_loss = running_loss / num_batches\n",
    "\n",
    "    epoch_time = time.time() - start_epoch\n",
    "    t.set_description(f\"Epoch {epoch + 1}\")\n",
    "    t.set_postfix(epoch_time=epoch_time, training_loss=avg_train_loss, refresh=True)\n",
    "\n",
    "# 3. Semi-supervised learning - pseudo-labeling\n",
    "# Generate pseudo-labels for the unlabeled data\n",
    "#unlabeled_dataset = VideoDataset(\"/scratch/dks7920/Dataset_Student/unlabeled\")\n",
    "#unlabeled_dataloader = DataLoader(unlabeled_dataset, batch_size=32, shuffle=False)\n",
    "segmentation_model.eval()\n",
    "pseudo_labels = []\n",
    "with torch.no_grad():\n",
    "    for videos in small_unlabeled_dataloader:\n",
    "        videos = videos.to(device)\n",
    "        predictions = segmentation_model(videos[:, :11]) # Use first 11 frames\n",
    "        pseudo_labels.append(predictions.argmax(dim=1).cpu())\n",
    "        \n",
    "combined_dataset = ConcatDataset([labeled_train_dataset] + [TensorDataset(videos, masks) for videos, masks in zip(unlabeled_dataset, pseudo_labels)])\n",
    "combined_dataloader = DataLoader(combined_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    # Training loop\n",
    "    segmentation_model.train()\n",
    "    for videos, masks in train_dataloader:\n",
    "        videos = videos.to(device)\n",
    "        masks = masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = segmentation_model(videos)\n",
    "        loss = criterion(predictions, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    avg_train_loss = running_loss / num_batches\n",
    "\n",
    "    # Validation loop\n",
    "    segmentation_model.eval()\n",
    "    val_loss = 0\n",
    "    iou_sum = 0\n",
    "    num_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for videos, masks in val_dataloader:\n",
    "            videos = videos.to(device)\n",
    "            masks = masks.to(device)\n",
    "            predictions = segmentation_model(videos)\n",
    "            loss = criterion(predictions, masks)\n",
    "            val_loss += loss.item()\n",
    "            iou_sum += calculate_iou_batch(predictions, masks)\n",
    "            num_batches += 1\n",
    "\n",
    "    # Calculate average validation loss and IoU\n",
    "    avg_val_loss = val_loss / num_batches\n",
    "    avg_iou = iou_sum / num_batches\n",
    "\n",
    "    # Update the progress bar with training and validation information\n",
    "    t.set_description(f\"Epoch {epoch + 1}\")\n",
    "    t.set_postfix(\n",
    "        train_loss=f\"{avg_train_loss:.4f}\",\n",
    "        val_loss=f\"{avg_val_loss:.4f}\",\n",
    "        iou=f\"{avg_iou:.4f}\",\n",
    "        refresh=True,\n",
    "    )\n",
    "    if avg_iou > best_iou:\n",
    "        best_iou = avg_iou\n",
    "        torch.save(segmentation_model.state_dict(), f\"best_model.pth\")\n",
    "        t.write(f\"Best model saved at Epoch {epoch + 1} with IOU: {avg_iou:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep Learning",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
