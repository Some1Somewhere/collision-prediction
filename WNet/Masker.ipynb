{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4sI0XNmTUEy"
   },
   "source": [
    "# Masker Model\n",
    "\n",
    "LÃ©o Dupire"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "lCg2FAji3e-k"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4482,
     "status": "ok",
     "timestamp": 1682630520071,
     "user": {
      "displayName": "Leo Dupire",
      "userId": "02301275893739117839"
     },
     "user_tz": 240
    },
    "id": "rit-2JGTsHBL"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import tqdm.auto as tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import imageio.v3 as iio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## PyTorch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset\n",
    "from torchsummary import summary\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yl4JJbzsTTmy"
   },
   "outputs": [],
   "source": [
    "#@title Import Tensor Data\n",
    "# Train\n",
    "imgs = torch.load('data/imgs.pt')\n",
    "masks = torch.load('data/masks.pt')\n",
    "\n",
    "# Validation\n",
    "val_imgs = torch.load('data/val_imgs.pt')\n",
    "val_masks = torch.load('data/val_masks.pt')\n",
    "\n",
    "# Print shapes of tensors\n",
    "print(\"Train imgs:\", imgs.shape)\n",
    "print(\"Val imgs:\", val_imgs.shape)\n",
    "print()\n",
    "print(\"Train masks:\", masks.shape)\n",
    "print(\"Val masks:\", val_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CkCNh8RLqIQr"
   },
   "outputs": [],
   "source": [
    "# Display an Image-Mask pair from the validation set\n",
    "fig, axes = plt.subplots(1, 2, figsize=(6.4*2, 4.8))\n",
    "axes[0].imshow(val_imgs[0][0])\n",
    "axes[1].imshow(val_masks[0][0])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4reD9avhamIp"
   },
   "source": [
    "## Dataset Augmentation and Loaders"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Train & Val:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Zzw7yekcXIsc"
   },
   "outputs": [],
   "source": [
    "#@title Dataset Class & Loaders\n",
    "class MaskDataset(Dataset):\n",
    "  def __init__(self, imgs, masks, transform=None):\n",
    "    self.imgs = imgs.reshape(-1, 160, 240, 3)\n",
    "    self.masks = masks.reshape(-1, 160, 240)\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.masks)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    img = self.imgs[index].to(torch.uint8)\n",
    "    msk = self.masks[index]\n",
    "    img = img.permute(2, 0, 1).to(torch.float) / 255\n",
    "    img = (img - 0.5) / 2\n",
    "\n",
    "    if random.random() > 0.5: # Random Horizontal Flip on both image and corresponding mask\n",
    "      img = torch.flip(img, dims=[2])\n",
    "      msk = torch.flip(msk, dims=[1])\n",
    "\n",
    "    return (img, msk)\n",
    "\n",
    "# Datasets\n",
    "train_dataset = MaskDataset(imgs, masks,)\n",
    "val_dataset = MaskDataset(val_imgs, val_masks,)\n",
    "\n",
    "# Data Loaders\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, drop_last=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1TrDeKW0qgoX"
   },
   "source": [
    "## UNet - Masker\n",
    "\n",
    "Code inspired by: https://pyimagesearch.com/2021/11/08/u-net-training-image-segmentation-models-in-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "form",
    "id": "XpL9h5Z3qjjQ"
   },
   "outputs": [],
   "source": [
    "#@title Conv Block\n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_c)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_c)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "form",
    "id": "hcJI0pm-qoM7"
   },
   "outputs": [],
   "source": [
    "#@title Encoder Block\n",
    "class encoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = conv_block(in_c, out_c)\n",
    "        self.pool = nn.MaxPool2d((2, 2))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        p = self.pool(x)\n",
    "\n",
    "        return x, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "form",
    "id": "ag72YBH2q9tU"
   },
   "outputs": [],
   "source": [
    "#@title Decoder Block\n",
    "class decoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv = conv_block(out_c+out_c, out_c)\n",
    "\n",
    "    def forward(self, inputs, skip):\n",
    "        x = self.up(inputs)\n",
    "        x = torch.cat([x, skip], axis=1)\n",
    "        x = self.conv(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "form",
    "id": "yeYEGDt7q__L"
   },
   "outputs": [],
   "source": [
    "#@title UNet Block\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.e1 = encoder_block(3, 64)\n",
    "        self.e2 = encoder_block(64, 128)\n",
    "        self.e3 = encoder_block(128, 256)\n",
    "        self.e4 = encoder_block(256, 512)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.b = conv_block(512, 1024)\n",
    "\n",
    "        # Decoder\n",
    "        self.d1 = decoder_block(1024, 512)\n",
    "        self.d2 = decoder_block(512, 256)\n",
    "        self.d3 = decoder_block(256, 128)\n",
    "        self.d4 = decoder_block(128, 64)\n",
    "\n",
    "        # Classifier\n",
    "        self.outputs = nn.Conv2d(64, 49, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Encoder\n",
    "        s1, p1 = self.e1(inputs)\n",
    "        s2, p2 = self.e2(p1)\n",
    "        s3, p3 = self.e3(p2)\n",
    "        s4, p4 = self.e4(p3)\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.b(p4)\n",
    "\n",
    "        # Decoder\n",
    "        d1 = self.d1(b, s4)\n",
    "        d2 = self.d2(d1, s3)\n",
    "        d3 = self.d3(d2, s2)\n",
    "        d4 = self.d4(d3, s1)\n",
    "\n",
    "        # Classifier\n",
    "        outputs = self.outputs(d4)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Option 1) Create a New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1254,
     "status": "ok",
     "timestamp": 1682362777204,
     "user": {
      "displayName": "Leo Dupire",
      "userId": "02301275893739117839"
     },
     "user_tz": 240
    },
    "id": "trkQer_MXREH",
    "outputId": "2ab1d6fd-c2ce-448e-c34f-62a449247edc"
   },
   "outputs": [],
   "source": [
    "#@title New Model\n",
    "model = UNet().to(device)\n",
    "\n",
    "# Test forward pass\n",
    "input_tensor = train_dataset[0][0].unsqueeze(0).to(device) \n",
    "output = model(input_tensor)\n",
    "print(output.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Option 2) Load a Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellView": "form",
    "id": "O1JuWDz-xIhL"
   },
   "outputs": [],
   "source": [
    "#@title Load a Model\n",
    "model = UNet().to(device)\n",
    "model.load_state_dict(torch.load(\"./masker_models/masker.pth\"))\n",
    "\n",
    "# Test forward pass\n",
    "input_tensor = train_dataset[0][0].unsqueeze(0).to(device) \n",
    "output = model(input_tensor)\n",
    "print(output.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "KhkY6H18-WU9"
   },
   "outputs": [],
   "source": [
    "summary(model, input_size=(3, 160, 240))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-JRhklV85UOi"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "isKs68sM36x1"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-4) # You can even start at lr=1e-3\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=3, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "result = {\"train\": [], \"val\": []} # For tracking loss\n",
    "best_val = -1 # Negative as real val will be positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "U0SRgwWFEYU1"
   },
   "outputs": [],
   "source": [
    "#@title Display Mask Output Comparison\n",
    "def display_comp(model, index, ds=\"val\", show_img=False):\n",
    "  if ds == \"val\":\n",
    "    example_image, example_mask = val_dataset[index]\n",
    "  elif ds == \"train\":\n",
    "    example_image, example_mask = train_dataset[index]\n",
    "\n",
    "  pred_mask = model(example_image.unsqueeze(0).to(device)).cpu().squeeze(0)\n",
    "  \n",
    "  if show_img:\n",
    "    example_image = (((example_image.permute(1, 2, 0)*2) + 0.5) * 255).to(int)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 6))\n",
    "    axes[0].imshow(example_image, vmin=0, vmax=48)\n",
    "    axes[1].imshow(example_mask, vmin=0, vmax=48)\n",
    "    axes[2].imshow(pred_mask.argmax(0), vmin=0, vmax=48)\n",
    "  else:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
    "    axes[0].imshow(example_mask, vmin=0, vmax=48)\n",
    "    axes[1].imshow(pred_mask.argmax(0), vmin=0, vmax=48)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ALlc7TGBXTkZ"
   },
   "outputs": [],
   "source": [
    "#@title Training\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "# Loss + Update Model function\n",
    "def get_loss(image, mask, optimizer=None):\n",
    "  pred_mask = model(image)\n",
    "  loss = criterion(pred_mask, mask.long())\n",
    "\n",
    "  if optimizer is not None:\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  return loss\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(1, num_epochs+1), leave=False): # Train on several epochs\n",
    "  total_train_loss = 0\n",
    "  model.train()\n",
    "  for batch in tqdm(train_loader, leave=False):\n",
    "    image, mask = [x.to(device) for x in batch]\n",
    "    total_train_loss += get_loss(image, mask, optimizer=optimizer) # Get the loss and update\n",
    "\n",
    "  train_loss = total_train_loss.item() / len(train_loader)\n",
    "  result[\"train\"].append(train_loss) # Record loss for post-training visualization\n",
    "\n",
    "  # Test on validation\n",
    "  with torch.no_grad():\n",
    "    val_result = 0\n",
    "    count = 0\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    for batch in val_loader:\n",
    "      image, mask = [x.to(device) for x in batch]\n",
    "      total_val_loss += get_loss(image, mask) * image.size(0) # Get the loss\n",
    "      count += image.size(0)\n",
    "\n",
    "    val_result = total_val_loss.item() / count\n",
    "    result[\"val\"].append(val_result) # Record loss for post-training visualization\n",
    "    print(f\"Epoch {epoch} | Train: {train_loss:.4f} | Val: {val_result:.4f}\") # Print Epoch losses\n",
    "\n",
    "    if (best_val == -1) or (val_result < best_val):\n",
    "        best_val = val_result\n",
    "        torch.save(model.state_dict(), \"./masker_models/best_masker.pth\") # Save best model\n",
    "\n",
    "    display_comp(model, 40) # Display prediction example on validation (function defined in cell above)\n",
    "  scheduler.step(total_val_loss) # Send validation loss to lr_scheduler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SsftTNTOIyFT"
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "executionInfo": {
     "elapsed": 388,
     "status": "ok",
     "timestamp": 1682349353597,
     "user": {
      "displayName": "Leo Dupire",
      "userId": "02301275893739117839"
     },
     "user_tz": 240
    },
    "id": "jz-gnbD-BSIr",
    "outputId": "57f73864-e6b6-4522-98fc-9a8462714306"
   },
   "outputs": [],
   "source": [
    "# View Training Progress\n",
    "fig_prog = plt.figure(figsize=(6,4))\n",
    "\n",
    "plt.plot(range(1, len(result[\"train\"])+1), result[\"train\"], label=\"Train\")\n",
    "plt.plot(range(1, len(result[\"val\"])+1), result[\"val\"], label=\"Val\")\n",
    "plt.title(\"Reconstruction error over epoch\", fontsize=14)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 1183,
     "status": "ok",
     "timestamp": 1682349372966,
     "user": {
      "displayName": "Leo Dupire",
      "userId": "02301275893739117839"
     },
     "user_tz": 240
    },
    "id": "67lItzoVXVw5",
    "outputId": "f54bd5ec-7b8e-415f-e028-fa2655341c95"
   },
   "outputs": [],
   "source": [
    "# View a specific example\n",
    "example_num = 4\n",
    "model.eval()\n",
    "display_comp(model, example_num, ds=\"val\", show_img=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Masks for Unlabeled Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Unlabeled: i.e. _Lazy Loading_ (Optional)\n",
    "\n",
    "__Only to be run once the Masker is fully trained!__ We will use the Masker to generate masks for the unlabeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = \"./data/Dataset_Student\" # Unlabeled data directory\n",
    "\n",
    "# Get sorted list of videos in unlabeled folder\n",
    "dir_list = os.listdir(f\"{P}/unlabeled/\")\n",
    "lst1 = [x for x in dir_list if len(x) == 10]\n",
    "lst2 = [x for x in dir_list if len(x) == 11]\n",
    "lst1.sort()\n",
    "lst2.sort()\n",
    "dirs = lst1 + lst2\n",
    "len(dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyDataset(Dataset):\n",
    "    def __init__(self, dir_list=None):\n",
    "        self.data_files = dir_list # Loads a video folder\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        video = []\n",
    "        for i in range(22): # Extract all images from corresponding video folder\n",
    "            img = torch.Tensor(np.copy(iio.imread(f\"{P}/unlabeled/{self.data_files[index]}/image_{i}.png\"))).to(torch.uint8)\n",
    "            img = img.permute(2, 0, 1).to(torch.float) / 255\n",
    "            img = (img - 0.5) / 2\n",
    "            video.append(img)\n",
    "        video_imgs = torch.stack(video)\n",
    "        \n",
    "        return video_imgs\n",
    "\n",
    "unlabeled_dataset = LazyDataset(dir_list=dirs)\n",
    "unlabeled_loader = torch.utils.data.DataLoader(unlabeled_dataset, num_workers=8) # Lazy Loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate & Save Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'empty' tensor\n",
    "unlabeled_masks = torch.zeros([13000, 22, 160, 240]) # For rough memory availability verification\n",
    "unlabeled_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Unlabeled Images\n",
    "count = 0\n",
    "model.eval()\n",
    "for batch in tqdm.tqdm(unlabeled_loader): # Go through all examples\n",
    "    inp = batch.squeeze(0).to(device)\n",
    "    masks = model(inp)\n",
    "    unlabeled_masks[count] = masks.argmax(1).unsqueeze(0) # Record in order\n",
    "    count += 1 # Keep track of order (index)\n",
    "\n",
    "torch.save(unlabeled_masks, '/data/unlabeled_masks.pt') # Save unlabeled masks as 'unlabeled_masks.pt' in ~/WNet/data"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "lCg2FAji3e-k",
    "4reD9avhamIp",
    "1TrDeKW0qgoX",
    "-JRhklV85UOi",
    "SsftTNTOIyFT"
   ],
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
